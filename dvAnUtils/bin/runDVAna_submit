#!/usr/bin/env python
""":script:`runDVAna_submit` -- Prepare runDVAna jobs to be submitted in a batch 
================================================================================

.. script:: runDVAna_submit
  :platform: unix
  :synopsis:     Prepare the suitables inputs needed to submit local batch 
                 jobs of the  runDVAna executable using the clustermanager
                 script. 
  :inputs:       DATASETS which should be available in the AMI db
  :dependencies: FAX (setupATLAS && lsetup fax)
.. author:: jordi duarte-campderros <jorge.duarte.campderros@cern.ch>
"""
# TO BE PROMOTED
from contextlib import contextmanager
import os

@contextmanager
def cd(newdir):
    """Always returns to the previous directory
    """
    prevdir = os.getcwd()
    os.chdir(os.path.expanduser(newdir))
    try:
        yield
    finally:
        os.chdir(prevdir)

def getLFNd_and_Events(scope,dataset,verbose=False):
    """Obtain the Logical file names belonging to a dataset

    Parameters
    ----------
    scope: str
    dataset: srt

    verbose: bool [False]

    Returns
    -------
    collFiles: dict((str,int))
        list of files and events contained in that file

    Notes
    -----
    Inspired in fax-get-gLFNs
    """
    import os
    try:
        import rucio.client
    except ImportError:
        raise ImportError("Please do first: 'lsetup fax'")
    
    # get redirector
    if verbose:
        print '\033[1;34mINFO\033[1;m Seeking available files for "{0}:{1}"'.format(scope,dataset)
    redir = os.environ.get("STORAGEPREFIX")
    if not redir:
        raise RuntimeError("no FAX redirector given. Please do: 'lsetup fax'")
    filename = lambda scope,fname: "{0}/atlas/rucio/{1}:{2}".format(redir,scope,fname)

    didcli = rucio.client.didclient.DIDClient()
    cont = didcli.list_files(scope,dataset)
    collFiles = map(lambda f: (filename(f['scope'],f['name']),f['events']), cont)
    return dict(collFiles)

def get_tree_entries(_file):
    """Function called by each worker in the get_event_per_file
    function

    Parameters
    ----------
    _file: str
        the name of the root file
    
    Returns
    -------
    evts: int
        the Number of events
    """
    import ROOT
    f = ROOT.TFile.Open(_file)
    if f.IsZombie():
        print 
        print "\033[1;33mWARNING\033[1;m file '{0}' not found".format(_file)
        return 0
    evts = f.CollectionTree.GetEntries()
    f.Close()
    del f
    return evts

def get_event_per_file(folder,files,scope,dsname):
    """Extract the number of events for each file and store it as list
    in the folder where the files are. 

    Parameters
    ----------
    folder: str
        path under which the files are living
    files: str
        relative name of the files
    scope: str
        the scope of the dataset (see ATLAS datasets format)
    dsname: str
        the name of the dataset (see ATLAS dataset format)

    Return
    ------
    final_result: list( (str,int) )
        the list of number of events per file

    Notes
    -----
    This function opens in parallel as many files as it can (using all the 
    cores availables) in order to minimize the evaluation of the number of
    events per file when there are a huge amount of files. 
    """
    import multiprocessing
    import shelve
    import sys
    import os
    import time

    # the name of the file with the event info
    print "\033[1;34mINFO\033[1;m Getting the number of events per file..."
    dbfile_name = os.path.join(folder,".events_per_file")
    dbfile = shelve.open(dbfile_name,writeback=True)
    try:
        stored_collFiles = dbfile['evt_file_dict']
        # just to be sure (TO BE DEPRECATED sooon)
        stored_collFiles = dict(stored_collFiles) 
        print "\033[1;34mINFO\033[1;m Found a database with this info..."
    except KeyError:
        # empty file, let's fill it using rucio
        pre_collFiles = getLFNd_and_Events(scope,dsname)
        # just use the right folder, note also that the previous
        # function insert the scope (:) before the name of the file
        stored_collFiles = dict(map(lambda (fn,e): 
                (os.path.join(folder,os.path.basename(fn).split(":")[-1]),e), pre_collFiles.iteritems()))
    # Just using those which were introduced by the user
    collFiles_files = filter(lambda x: x in files, stored_collFiles.keys())
    # Let's see if the number of files are correct or it is needed an update
    missing_in_db = list(set(files).difference(collFiles_files))
    # need to be updated the DB file
    if len(missing_in_db) > 0:
        ncpu = multiprocessing.cpu_count()
        pool = multiprocessing.Pool(ncpu)

        collFiles = {}
        for i in xrange(len(missing_in_db)):
            evts_result = pool.map_async(get_tree_entries,[missing_in_db[i]])
            collFiles[missing_in_db[i]] = evts_result
        pool.close()
        # -- Work dispatched, now waiting until all finished
        #    Print something on the screen, don't let the user think that
        #    we're not doing anything

        # progress bar ...
        final_result = {}
        pointpb = float(len(collFiles))/100.0
        while True:
            # get the number of running processes
            # re-evaluating every loop (not a big deal...)
            files_ready = 0
            evts_added  = 0
            for x,res in collFiles.iteritems():
                try:
                    _dum = res.ready()
                    if _dum:
                        res = res.get()[0]
                        # just catched by the exception
                        raise AttributeError
                except AttributeError:
                    evts_added += res
                    files_ready += 1
                    # be sure the final list contain event numbers only
                    final_result[x] = res
            sys.stdout.write("\r\033[1;34mINFO\033[1;m Obtaining number of events"+\
                    " from the files [ "+"\b"+str(int(float(files_ready)/pointpb)+1).rjust(3)+\
                    "%] ::: \033[1;34mFILES PROCESSED\033[1;m:"+str(files_ready)+" \033[1;34mEVENTS\033[1;m:"+str(evts_added))
            sys.stdout.flush()
            if len(collFiles) == files_ready: 
                # recover the expected format of the list, 
                # without using the multiprocessing result object
                collFiles = final_result
                break
            time.sleep(0.2)
        print
        pool.join()
    else:
        # just copy back 
        collFiles = stored_collFiles
    # store back
    print "\033[1;34mINFO\033[1;m Storing result in database to avoid a future recalculation..."
    dbfile['evt_file_dict'] = collFiles
    dbfile.close()
    return collFiles


def getLocalFiles_and_Events(listfile,scope,dsname):
    """Obtain the number of events per file

    Parameters
    ----------
    files: str
        file with the list of input files to use

    Returns
    -------
    collFiles: dict((str,int))
        dict of files and events contained in that file
    """
    from ROOT import TFile,TChain
    import sys
    import os

    #collFiles= []
    with open(listfile) as _f1:
        files_str = _f1.read()
        _f1.close()
    # to a list (with no empty strings)
    files = filter(None,files_str.split("\n"))

    # Obtain the list of files from a previously used file
    # or if not create those file
    folder_files =  os.path.split(files[0])[0]
    collFiles = get_event_per_file(folder_files,files,scope,dsname)
    
    return collFiles

def files_per_job(fdict,evtperjob):
    """Algorithm to split a list of n-files into a minimum of evts per job
    The algorithm will try to match the required number of events per job, but
    the input files are the atoms of the scheme, so they cannot be divided.
    
    Parameters
    ----------
    fdict: dict( (str,int) )
        dict with the files with their corresponding number of events.
        This list can be obtained directly from the 'getLFNd_and_Events'
        function

    evtperjob: int
        the recommended number of events per job

    Return
    ------
    files_jobid: dict( int: [str] )
        the list of files (values) to be processed by each job (key)
    """
    i = 0
    files_jobid = {}
    evts_in_this_job=0
    for fname,evt in fdict.iteritems():
        try:
            files_jobid[i].append(fname)
        except KeyError:
            files_jobid[i] = [fname]
        evts_in_this_job+=evt
        if evts_in_this_job >= evtperjob:
            evts_in_this_job=0
            i+=1
    # Just checking the coherence...  
    if sum(map(lambda ylist: len(ylist),files_jobid.values())) != len(fdict):
        raise RuntimeError("INTERNAL ERROR: The number of files collected in the"\
                " files_jobid dict (which are the values) must be the same"\
                " than the number of introduced files.")
    return files_jobid

def create_auxiliary_file(jobid,filelist,filename="filelist"):
    """Create a file with the 'filelist' content. This function
    intends to create the input file list which are going to feed 
    each job. The created file will be: {0}_{1}.txt.format(jobid,filename)

    Parameters
    ----------
    jobid: str
    filelist: str
        space separated list of files
    filename: str, [filelist]

    Return
    ------
    the name of the created file
    """
    lines = "{0}\n".format(' '.join(filelist))
    current_filename = "{0}_{1}.txt".format(filename,jobid)
    with open(current_filename,"w") as f:
        f.write(lines)
        f.close()
    return current_filename

def create_bash(filelist,testarea,bashname,algorithm,outputname):
    """Create a generic bashscript allowed to be used for each job,
    so it contains some wildcards (%i) which the clustermanager (type:blind)
    is going to use to define the jobs. The execute permission are set 
    as well.

    Parameters
    ----------
    filelist: str
        space separated list of the input (root) files
    testarea: str
        the path to the area where is installed RootCore/DV_xAODAnalysis
    bashname: str
        name of the bash script (with n extension), so it creates 'bashname.sh'
    algorithm: str
        name of the DVAnalyses algorithm (to feed the -a option of runDVAna)
    outputname: str
        the generic name of the runDVAna output files 
    """
    import time,datetime
    import os
    import random

    ts = time.time()
    timestamp = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')
    bashfile = '#!/bin/bash\n\n'
    bashfile += '# File created by the script "runDVAna_submit" [{0}]\n'.format(timestamp)
    bashfile += '#'+'='*80+'\n'
    bashfile += 'hostname\n'
    bashfile += 'pwd\n'
    bashfile += '#'+'='*80+'\n'
    bashfile += 'source $ATLAS_LOCAL_ROOT_BASE/user/atlasLocalSetup.sh; \n'
    bashfile += 'cd '+testarea+' ;\n'
    bashfile += "lsetup 'rcsetup' ;\n"
    # Issue with the directory, create a temporary one
    bashfile += 'tmpdir=$(mktemp -d) ;\n'
    bashfile += 'cd ${tmpdir};\n'
    bashfile += 'runDVAna -a {3} -o {0} -f `cat {1}/{2};` ;\n'.format(
            '{0}_%i.root'.format(outputname),os.getcwd(),filelist,algorithm)
    bashfile += 'mv {0}_%i.root {1}/ ;\n'.format(outputname,os.getcwd())
    bashfile += 'rm -rf ${tmpdir} ;\n'
    scriptname = '{0}.sh'.format(bashname)
    f = open(scriptname,"w")
    f.write(bashfile)
    f.close()
    os.chmod(scriptname,0755)

def main(DSs,testarea,localfiles,algorithm,outDSs,version,njobs=200,events_per_job=None):
    """Steer the creation of the jobs. Per each dataset it will
        1. Obtain the list of files
        2. Create a directory 
        3. Populate the directory with a bashscript and files with 
           the list of input files per job
        4. Remind the clustermanager command to be sent inside the
           directory

    After that use the command printed to actual send the job to the
    cluster
    """
    import os
    
    # for each dataset 
    for ds,outDS in zip(DSs,outDSs):
        scope,dsname = ds.split(":")
        splitname = dsname.split('.')
        if not outDS:
            name = "{0}.{1}.{2}".format(splitname[3],splitname[2],splitname[8])
        else:
            name = outDS
        print "\033[1;34mINFO\033[1;m Creating job for {0}".format(name)
        # -- create a directory to launch
        directory=os.path.join(os.getcwd(),name)
        if not os.path.exists(directory):
            os.makedirs(directory)
        # get the name of the files to process and events per file
        fdict = getLocalFiles_and_Events(localfiles,scope,dsname)
        with cd(directory):
            # get the events per jobs
            totalevts = sum(map(lambda x: x,fdict.values()))
            evtperjob = totalevts/njobs
            remainevts= totalevts%njobs
            # build the list
            jobid_files = files_per_job(fdict,evtperjob)
            # create an unique bash script 
            filelist_name = 'filelist_%i.txt'
            jobname = '{0}.{1}.v{2}'.format(name,algorithm,version)
            outputname = "{0}_V{1}".format(algorithm,version)

            create_bash(filelist_name,testarea,jobname,algorithm,outputname)
            # create the dv-analysis jobs
            for jobid,filelist in jobid_files.iteritems():
                auxiliary_filename = create_auxiliary_file(jobid,filelist,filename=filelist_name.split('_')[0])
            print "\033[1;32mCOMMAND\033[1;m clustermanager send -t blind -b {0} --specific-file {1} "\
                    "-n {2}".format(jobname,filelist_name.split('_')[0],njobs)
            #FIXME: do it right away!
    print "Send the jobs to the cluster!!"

if __name__ == '__main__':
    from optparse import OptionParser
    import os
    from os.path import expanduser

    usage  = "usage: submit_localbatch.py DS1 DS2 ... [options]"
    parser = OptionParser(usage=usage)
    parser.add_option( "-a",action='store',dest='algorithm',\
            help="the DVAnalyses algorithm (the -a option of runDVAna) [BasicPlots]",\
            default="{0}".format('BasicPlots'))
    parser.add_option( "--outDS",action='store',dest='outDS',\
            help="the Dataset output names, if not given it creates a name based in the DS [None]",default=None)
    parser.add_option( "-t",action='store',dest='testarea',\
            help="the path to the test area [$HOME/work/private/DualUse]",\
            default="{0}/work/private/DualUse".format(expanduser("~")))
    parser.add_option( "-j",action='store',dest='njobs',help="the number of jobs",\
            default=200)
    parser.add_option( "-l","--local-files",action='store',dest='localfiles',\
            metavar="LISTFILES.txt",help="the input files are located locally"\
            " and listed into LISTFILES.txt filed. With this option, it is only"\
            " valid to process one DS")
    parser.add_option( "--process-version",action='store',dest='version',\
            help="useful whenever the same job should be re-sent [0]",\
            default=0)

    (opt,args) = parser.parse_args()
    if len(args) < 1:
        message = "\033[1;31msubmit_localbatch ERROR\033[1;m Missing list input datasets"
        raise RuntimeError(message)
    import sys

    # check that we have the same number of outDS than DS, otherwise raise an error
    if not opt.outDS:
        # same number of outDS than DSs (empty in that case)
        outDSs = [None]*len(args)
    elif len(opt.outDS.split(",")) != len(args):
        # need to be the same number
        raise RuntimeError("The same number of outDS ('--outDS') are needed than the 'DS'")
    else:
        outDSs = opt.outDS.split(",")
    main(args,opt.testarea,opt.localfiles,opt.algorithm,outDSs,opt.version,njobs=int(opt.njobs))


    


