#!/usr/bin/env python
""":script:`runDVAna_submit` -- Prepare runDVAna jobs to be submitted in a batch 
================================================================================

.. script:: runDVAna_submit
  :platform: unix
  :synopsis:     Prepare the suitables inputs needed to submit local batch 
                 jobs of the  runDVAna executable using the clustermanager
                 script. 
  :inputs:       DATASETS which should be available in the AMI db
  :dependencies: FAX (setupATLAS && lsetup fax)
.. author:: jordi duarte-campderros <jorge.duarte.campderros@cern.ch>
"""

# =============================================================================
# CLUSTER RELATED FUNCTIONS

# TO BE PROMOTED
from contextlib import contextmanager
import os

@contextmanager
def cd(newdir):
    """Always returns to the previous directory
    """
    prevdir = os.getcwd()
    os.chdir(os.path.expanduser(newdir))
    try:
        yield
    finally:
        os.chdir(prevdir)

def getLFNd_and_Events(scope,dataset,verbose=False):
    """Obtain the Logical file names belonging to a dataset

    Parameters
    ----------
    scope: str
    dataset: srt

    verbose: bool [False]

    Returns
    -------
    collFiles: dict((str,int))
        list of files and events contained in that file

    Notes
    -----
    Inspired in fax-get-gLFNs
    """
    import os
    try:
        import rucio.client
    except ImportError:
        raise ImportError("Please do first: 'lsetup fax'")
    
    # get redirector
    if verbose:
        print '\033[1;34mINFO\033[1;m Seeking available files for "{0}:{1}"'.format(scope,dataset)
    redir = os.environ.get("STORAGEPREFIX")
    if not redir:
        raise RuntimeError("no FAX redirector given. Please do: 'lsetup fax'")
    filename = lambda scope,fname: "{0}/atlas/rucio/{1}:{2}".format(redir,scope,fname)

    didcli = rucio.client.didclient.DIDClient()
    cont = didcli.list_files(scope,dataset)
    collFiles = map(lambda f: (filename(f['scope'],f['name']),f['events']), cont)
    return dict(collFiles)

def get_tree_entries(_file):
    """Function called by each worker in the get_event_per_file
    function

    Parameters
    ----------
    _file: str
        the name of the root file
    
    Returns
    -------
    evts: int
        the Number of events
    """
    import ROOT
    f = ROOT.TFile.Open(_file)
    if f.IsZombie():
        print 
        print "\033[1;33mWARNING\033[1;m file '{0}' not found".format(_file)
        return 0
    evts = f.CollectionTree.GetEntries()
    f.Close()
    del f
    return evts

def get_event_per_file(folder,files,scope,dsname):
    """Extract the number of events for each file and store it as list
    in the folder where the files are. 

    Parameters
    ----------
    folder: str
        path under which the files are living
    files: str
        relative name of the files
    scope: str
        the scope of the dataset (see ATLAS datasets format)
    dsname: str
        the name of the dataset (see ATLAS dataset format)

    Return
    ------
    final_result: list( (str,int) )
        the list of number of events per file

    Notes
    -----
    This function opens in parallel as many files as it can (using all the 
    cores availables) in order to minimize the evaluation of the number of
    events per file when there are a huge amount of files. 
    """
    import multiprocessing
    import shelve
    import sys
    import os
    import time

    # the name of the file with the event info
    print "\033[1;34mINFO\033[1;m Getting the number of events per file..."
    dbfile_name = os.path.join(folder,".events_per_file")
    dbfile = shelve.open(dbfile_name,writeback=True)
    try:
        stored_collFiles = dbfile['evt_file_dict']
        # just to be sure (TO BE DEPRECATED sooon)
        stored_collFiles = dict(stored_collFiles) 
        print "\033[1;34mINFO\033[1;m Found a database with this info..."
    except KeyError:
        # empty file, let's fill it using rucio
        pre_collFiles = getLFNd_and_Events(scope,dsname)
        # just use the right folder, note also that the previous
        # function insert the scope (:) before the name of the file
        stored_collFiles = dict(map(lambda (fn,e): 
                (os.path.join(folder,os.path.basename(fn).split(":")[-1]),e), pre_collFiles.iteritems()))
    # Just using those which were introduced by the user
    collFiles_files = filter(lambda x: x in files, stored_collFiles.keys())
    # Let's see if the number of files are correct or it is needed an update
    missing_in_db = list(set(files).difference(collFiles_files))
    # need to be updated the DB file
    if len(missing_in_db) > 0:
        ncpu = multiprocessing.cpu_count()
        pool = multiprocessing.Pool(ncpu)

        collFiles = {}
        for i in xrange(len(missing_in_db)):
            evts_result = pool.map_async(get_tree_entries,[missing_in_db[i]])
            collFiles[missing_in_db[i]] = evts_result
        pool.close()
        # -- Work dispatched, now waiting until all finished
        #    Print something on the screen, don't let the user think that
        #    we're not doing anything

        # progress bar ...
        final_result = {}
        pointpb = float(len(collFiles))/100.0
        while True:
            # get the number of running processes
            # re-evaluating every loop (not a big deal...)
            files_ready = 0
            evts_added  = 0
            for x,res in collFiles.iteritems():
                try:
                    _dum = res.ready()
                    if _dum:
                        res = res.get()[0]
                        # just catched by the exception
                        raise AttributeError
                except AttributeError:
                    evts_added += res
                    files_ready += 1
                    # be sure the final list contain event numbers only
                    final_result[x] = res
            sys.stdout.write("\r\033[1;34mINFO\033[1;m Obtaining number of events"+\
                    " from the files [ "+"\b"+str(int(float(files_ready)/pointpb)+1).rjust(3)+\
                    "%] ::: \033[1;34mFILES PROCESSED\033[1;m:"+str(files_ready)+" \033[1;34mEVENTS\033[1;m:"+str(evts_added))
            sys.stdout.flush()
            if len(collFiles) == files_ready: 
                # recover the expected format of the list, 
                # without using the multiprocessing result object
                collFiles = final_result
                break
            time.sleep(0.2)
        print
        pool.join()
    else:
        # just copy back 
        collFiles = stored_collFiles
    # store back
    print "\033[1;34mINFO\033[1;m Storing result in database to avoid a future recalculation..."
    dbfile['evt_file_dict'] = collFiles
    dbfile.close()
    return collFiles


def getLocalFiles_and_Events(listfile,scope,dsname):
    """Obtain the number of events per file

    Parameters
    ----------
    files: str
        file with the list of input files to use

    Returns
    -------
    collFiles: dict((str,int))
        dict of files and events contained in that file
    """
    from ROOT import TFile,TChain
    import os

    #collFiles= []
    with open(listfile) as _f1:
        files_str = _f1.read()
        _f1.close()
    # to a list (with no empty strings)
    files = filter(None,files_str.split("\n"))

    # Obtain the list of files from a previously used file
    # or if not create those file
    folder_files =  os.path.split(files[0])[0]
    collFiles = get_event_per_file(folder_files,files,scope,dsname)
    
    return collFiles

def files_per_job(fdict,evtperjob):
    """Algorithm to split a list of n-files into a minimum of evts per job
    The algorithm will try to match the required number of events per job, but
    the input files are the atoms of the scheme, so they cannot be divided.
    
    Parameters
    ----------
    fdict: dict( (str,int) )
        dict with the files with their corresponding number of events.
        This list can be obtained directly from the 'getLFNd_and_Events'
        function

    evtperjob: int
        the recommended number of events per job

    Return
    ------
    files_jobid: dict( int: [str] )
        the list of files (values) to be processed by each job (key)
    """
    i = 0
    files_jobid = {}
    evts_in_this_job=0
    for fname,evt in fdict.iteritems():
        try:
            files_jobid[i].append(fname)
        except KeyError:
            files_jobid[i] = [fname]
        evts_in_this_job+=evt
        if evts_in_this_job >= evtperjob:
            evts_in_this_job=0
            i+=1
    # Just checking the coherence...  
    if sum(map(lambda ylist: len(ylist),files_jobid.values())) != len(fdict):
        raise RuntimeError("INTERNAL ERROR: The number of files collected in the"\
                " files_jobid dict (which are the values) must be the same"\
                " than the number of introduced files.")
    return files_jobid

def create_auxiliary_file(jobid,filelist,filename="filelist"):
    """Create a file with the 'filelist' content. This function
    intends to create the input file list which are going to feed 
    each job. The created file will be: {0}_{1}.txt.format(jobid,filename)

    Parameters
    ----------
    jobid: str
    filelist: str
        space separated list of files
    filename: str, [filelist]

    Return
    ------
    the name of the created file
    """
    lines = "{0}\n".format(' '.join(filelist))
    current_filename = "{0}_{1}.txt".format(filename,jobid)
    with open(current_filename,"w") as f:
        f.write(lines)
        f.close()
    return current_filename

def create_bash(filelist,testarea,bashname,algorithm,outputname):
    """Create a generic bashscript allowed to be used for each job,
    so it contains some wildcards (%i) which the clustermanager (type:blind)
    is going to use to define the jobs. The execute permission are set 
    as well.

    Parameters
    ----------
    filelist: str
        space separated list of the input (root) files
    testarea: str
        the path to the area where is installed RootCore/DV_xAODAnalysis
    bashname: str
        name of the bash script (with n extension), so it creates 'bashname.sh'
    algorithm: str
        name of the DVAnalyses algorithm (to feed the -a option of runDVAna)
    outputname: str
        the generic name of the runDVAna output files 
    """
    import time,datetime
    import os
    import random

    ts = time.time()
    timestamp = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')
    bashfile = '#!/bin/bash\n\n'
    bashfile += '# File created by the script "runDVAna_submit" [{0}]\n'.format(timestamp)
    bashfile += '#'+'='*80+'\n'
    bashfile += 'hostname\n'
    bashfile += 'pwd\n'
    bashfile += '#'+'='*80+'\n'
    bashfile += 'source $ATLAS_LOCAL_ROOT_BASE/user/atlasLocalSetup.sh; \n'
    bashfile += 'cd '+testarea+' ;\n'
    bashfile += "lsetup 'rcsetup' ;\n"
    # Issue with the directory, create a temporary one
    bashfile += 'tmpdir=$(mktemp -d) ;\n'
    bashfile += 'cd ${tmpdir};\n'
    bashfile += 'runDVAna -a {3} -o {0} -f `cat {1}/{2};` ;\n'.format(
            '{0}_%i.root'.format(outputname),os.getcwd(),filelist,algorithm)
    bashfile += 'mv {0}_%i.root {1}/ ;\n'.format(outputname,os.getcwd())
    bashfile += 'rm -rf ${tmpdir} ;\n'
    scriptname = '{0}.sh'.format(bashname)
    f = open(scriptname,"w")
    f.write(bashfile)
    f.close()
    os.chmod(scriptname,0755)

def main_cluster(DSs,testarea,localfiles,algorithm,outDSs,version,njobs=200,events_per_job=None):
    """Steer the creation of the jobs. Per each dataset it will
        1. Obtain the list of files
        2. Create a directory 
        3. Populate the directory with a bashscript and files with 
           the list of input files per job
        4. Remind the clustermanager command to be sent inside the
           directory

    After that use the command printed to actual send the job to the
    cluster
    """
    import os
    
    # for each dataset 
    for ds,outDS in zip(DSs,outDSs):
        scope,dsname = ds.split(":")
        splitname = dsname.split('.')
        if not outDS:
            name = "{0}.{1}.{2}".format(splitname[3],splitname[2],splitname[8])
        else:
            name = outDS
        print "\033[1;34mINFO\033[1;m Creating job for {0}".format(name)
        # -- create a directory to launch
        directory=os.path.join(os.getcwd(),name)
        if not os.path.exists(directory):
            os.makedirs(directory)
        # get the name of the files to process and events per file
        fdict = getLocalFiles_and_Events(localfiles,scope,dsname)
        with cd(directory):
            # get the events per jobs
            totalevts = sum(map(lambda x: x,fdict.values()))
            evtperjob = totalevts/njobs
            remainevts= totalevts%njobs
            # build the list
            jobid_files = files_per_job(fdict,evtperjob)
            # create an unique bash script 
            filelist_name = 'filelist_%i.txt'
            jobname = '{0}.{1}.v{2}'.format(name,algorithm,version)
            outputname = "{0}_V{1}".format(algorithm,version)

            create_bash(filelist_name,testarea,jobname,algorithm,outputname)
            # create the dv-analysis jobs
            for jobid,filelist in jobid_files.iteritems():
                auxiliary_filename = create_auxiliary_file(jobid,filelist,filename=filelist_name.split('_')[0])
            print "\033[1;32mCOMMAND\033[1;m clustermanager send -t blind -b {0} --specific-file {1} "\
                    "-n {2}".format(jobname,filelist_name.split('_')[0],njobs)
            #FIXME: do it right away!
    print "Send the jobs to the cluster!!"

# END --CLUSTER RELATED FUNCTIONS
# =============================================================================

# =============================================================================
# GRID RELATED FUNCTIONS
def main_prun(datasets,version,algorithm,multiple_ds,simulate=False):
    """
    """
    import datetime,time
    import getpass
    from subprocess import Popen,PIPE

    ts = time.time()
    timestamp = datetime.datetime.fromtimestamp(ts).strftime('%Y%m%d_%H%M%S')
    username = getpass.getuser()
    logfile="prunsubmission_{0}_{1}.log".format(timestamp,username)
    if simulate:
        print "\033[1;33mSIMULATING\033[1;m, not sending anything to the grid"
    else:
        print "\033[1;34mINFO\033[1;m sending {0} jobs to the grid".format(algorithm)
        print "\033[1;34mINFO\033[1;m log: {0} ".format(logfile)
    
    if multiple_ds:
        ds_list=[]

    # Get the datasets
    loglines = []
    # send all the input DS in one unique job
    pre_command= 'prun --exec=\"runDVAna -a {0} -o {1} -f %IN\" '\
            '--useRootCore --inDS={2} --outputs {1} --outDS {3} '\
            '--nGBPerJob MAX'
    pre_outDS = "user.{0}.{1}.{2}.{3}.{4}.v{5}"
    for ds in datasets:
        # extract a suitable name for the output
        try:
            # input DS with the repo_tab 
            _d1,_d2,DSnumber,physics_short,step,format_type,ami_tags,post_step,repo_tab,in_version = \
                    ds.split(":")[-1].split(".")
        except ValueError:
             # new 2016 data
             try:
                 _d1,_d2,DSnumber,physics_short,step,format_type,ami_tags,post_step,repo_tab,prV1,prV2 = \
                         ds.split(":")[-1].split(".")
                 in_version = prV1+"."+prV2
             except ValueError:
                 _d1,_d2,DSnumber,physics_short,step,format_type,ami_tags,post_step,in_version = \
                        ds.split(":")[-1].split(".")
                 # dummy repository tag
                 repo_tab="xxxx"

        outDS  = pre_outDS.format(username,physics_short,DSnumber,step,algorithm,version)
        rootname = "{0}_V{1}.root".format(algorithm,version)
        command= pre_command.format(algorithm,rootname,ds,outDS)
        
        if multiple_ds:
            ds_list.append( (ds,outDS) )
            # not send it
            continue
        
        if simulate:
            print command
            continue
        else:
            print "\033[1;34mINFO\033[1;m Sending {0}".format(outDS)
        p = Popen(command,stdout=PIPE,stderr=PIPE,shell=True).communicate()
        #if p[1] != "":
        #    message = "ERROR from prun:\n"
        #    message += p[1]+"\n"
        #    raise RuntimeError(message)
        # The error stream is not well defined in prun...
        loglines.append(p[1])
    
    if multiple_ds:
        unique_inDS = ','.join(map(lambda x: x[0], ds_list))
        # copy the ouDS but the DSnumber which it 
        unique_outDS_split = ds_list[0][1].split('.')
        # Extract the minimum and maximum DS number (only make sense in data)
        ## ---- note that is highly dependent of the inDS format, so it could
        ## ---- fail, in that case just put some dummy info
        nindex = 3
        foundit = False
        for i in xrange(len(ds_list[0][0].split('.'))):
            try: 
                isInt = int(ds_list[0][0].split('.')[i])
                nindex = i
            except ValueError:
                continue
            minDS = min(map(lambda x: x[0].split('.')[nindex],ds_list))
            maxDS = max(map(lambda x: x[0].split('.')[nindex],ds_list))
            foundit = True
            break
        if not foundit:
            minDS=0
            maxDS=1
        # rename the 3rd field (DS)
        unique_outDS_split[3] = '{0}_{1}'.format(minDS,maxDS)
        # the new name
        unique_outDS = '.'.join(unique_outDS_split)
        rootname = "{0}_{1}_{2}_V{3}.root".format(algorithm,minDS,maxDS,version)
        command= pre_command.format(algorithm,rootname,unique_inDS,unique_outDS)
        if simulate:
            print command
            return
        p = Popen(command,stdout=PIPE,stderr=PIPE,shell=True).communicate()
        loglines.append(p[1])
       
    if not simulate:
        with open(logfile,"w") as f1:
            f1.writelines(loglines)
            f1.close()

# END --GRID RELATED FUNCTIONS
# =============================================================================
if __name__ == '__main__':
    from argparse import ArgumentParser,RawDescriptionHelpFormatter
    import textwrap
    import os
    
    from optparse import OptionParser
    import os
    from os.path import expanduser
    
    usage  = "Submit a runDVAna jobs to a \033[1;54mcluster\033[1;m or "
    usage += "to the \033[1;54mgrid\033[1;m. The script\n"
    usage += "requires in both cases the use of a valid dataset name (DS) in the\n"
    usage += "standard format 'scope:dsname'"
    
    parser = ArgumentParser(prog='runDVAna_submit',
            formatter_class=RawDescriptionHelpFormatter,
            description=textwrap.dedent(usage))

    # Sub-command parsers
    subparsers = parser.add_subparsers(title='subcommands',
            description='valid subcommands', 
            help='additional help')

    # -- CLUSTER
    parser_cluster = subparsers.add_parser("cluster",help="Prepare jobs to be send them to the"\
            " local cluster. The jobs are not actually sent, so the user should do it (through"\
            " \033[1;54mclustermanager\033[1;m for instance")
    parser_cluster.add_argument("DSs",nargs="+",action='store',help="The Datasets in the scope:dsname"\
            " format")
    parser_cluster.add_argument( "-a",action='store',dest='algorithm',\
            help="the DVAnalyses algorithm (the -a option of runDVAna) [BasicPlots]",\
            default="{0}".format('BasicPlots'))
    parser_cluster.add_argument( "--outDS",action='store',dest='outDS',\
            help="the Dataset output names, if not given it creates a name based in the DS [None]",default=None)
    parser_cluster.add_argument( "-t",action='store',dest='testarea',\
            help="the path to the test area [$HOME/work/private/DualUse]",\
            default="{0}/work/private/DualUse".format(expanduser("~")))
    parser_cluster.add_argument( "-j",action='store',dest='njobs',help="the number of jobs",\
            default=200)
    parser_cluster.add_argument( "-l","--local-files",action='store',dest='localfiles',\
            metavar="LISTFILES.txt",help="the input files are located locally"\
            " and listed into LISTFILES.txt filed. With this option, it is only"\
            " valid to process one DS")
    parser_cluster.add_argument( "--process-version",action='store',dest='version',\
            help="useful whenever the same job should be re-sent [0]",\
            default=0)
    parser_cluster.set_defaults(which='cluster')
    
    # -- GRID
    parser_prun = subparsers.add_parser("prun",help="Uses the grid infrastructure"\
            " to send runDVAna jobs. Note that it requires to previously 'lsetup panda'")
    parser_prun.add_argument("DSs",nargs="+",action='store',help="The Datasets in the scope:dsname"\
            " format")
    parser_prun.add_argument( "-s",action='store_true',dest='simulate',default=False,\
            help="Don't send the jobs, just simulate and print the commands")
    parser_prun.add_argument( "-m","--multiple-datasets",action='store_true',dest='multiple_ds',default=False,\
            help="Send the whole bunch of datasets in an unique JOB set (--inDS DS1,DS2,DS3,...)")
    parser_prun.add_argument( "--step",action='store',dest='step',default="BasicPlots",\
            help="The algorithms(s) that are going to be run in the job, in order to"\
            " define the ouput Dataset name [BasicPlots]")
    parser_prun.add_argument( "-v","--version",action='store',dest='version',default="0",\
            help="The version number of the job")
    parser_prun.set_defaults(which="prun")

    args = parser.parse_args()

    if args.which == 'cluster':
        # check that we have the same number of outDS than DS, otherwise raise an error
        if not args.outDS:
            # same number of outDS than DSs (empty in that case)
            outDSs = [None]*len(args.DSs)
        elif len(args.outDS.split(",")) != len(args.DSs):
            # need to be the same number
            raise RuntimeError("The same number of outDS ('--outDS') are needed than the 'DS'")
        else:
            outDSs = args.outDS.split(",")
        main_cluster(args.DSs,args.testarea,args.localfiles,outDSs,args.version,njobs=int(args.njobs))
    elif args.which == 'prun':
        main_prun(args.DSs,args.version,args.step,args.multiple_ds,args.simulate)



    


